---
title:  "[논문 리뷰] DKM: Differentiable K-Means Clustering Layer for Neural Network Compression"
excerpt: "DKM: Differentiable K-Means Clustering Layer for Neural Network Compression"
categories: [Paper, Quantization]
tags: [quantization]

toc: true
toc_sticky: true
 
date: 2023-11-26 
last_modified_at: 2023-11-26 
---

## Info

이 논문은 Apple에서 출판된 논문으로 ICLR 2022에 발표되었다.

Quantization에 관심이 있어 공부하던 중 clustering을 적용하여 양자화하는 논문들이 존재해 하나씩 읽고 리뷰할 예정이다.

https://arxiv.org/pdf/2108.12659.pdf

## Abstract

효율적인 장치 내 추론을 위한 심층 신경망(DNN) 모델 압축은 메모리 요구 사항을 줄이고 사용자 데이터를 장치에 유지하기 위해 점점 더 중요해지고 있습니다. 이를 위해 우리는 새로운 미분 가능한 k-평균 클러스터링 계층(DKM)과 이를 열차 시간 가중치 클러스터링 기반 DNN 모델 압축에 적용하는 방법을 제안합니다. DKM은 k-평균 클러스터링을 주의 문제로 캐스팅하고 DNN 매개변수와 클러스터링 중심의 공동 최적화를 가능하게 합니다. 추가 정규화 도구 및 매개변수에 의존하는 이전 작업과 달리 DKM 기반 압축은 원래 손실 함수와 모델 아키텍처를 고정된 상태로 유지합니다. 컴퓨터 비전 및 자연어 처리(NLP) 작업을 위한 다양한 DNN 모델에서 DKM 기반 압축을 평가했습니다. 우리의 결과는 DKM이 ImageNet1k 및 GLUE 벤치마크에서 탁월한 압축 및 정확성 균형을 제공한다는 것을 보여줍니다. 예를 들어 DKM 기반 압축은 3.3MB 모델 크기(29.4x 모델 압축 계수)의 ResNet50 DNN 모델에서 74.5%의 상위 1 ImageNet1k 정확도를 제공할 수 있습니다. 압축하기 어려운 DNN인 MobileNet-v1의 경우 DKM은 0.72MB 모델 크기(22.4x 모델 압축 계수)로 63.9%의 상위 1 ImageNet1k 정확도를 제공합니다. 이 결과는 현재 최첨단 DNN 압축 알고리즘보다 상위 정확도가 6.8% 더 높고 모델 크기가 상대적으로 33% 더 작습니다. 또한 DKM은 GLUE NLP 벤치마크에서 정확도 손실을 최소화하면서(1.1%) DistilBERT 모델을 11.8배까지 압축할 수 있습니다.

> Deep neural network (DNN) model compression for efficient on-device inference is becoming increasingly important to reduce memory requirements and keep user data on-device. To this end, we propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight clustering-based DNN model compression. DKM casts k-means clustering as an attention problem and enables joint optimization of the DNN parameters and clustering centroids. Unlike prior works that rely on additional regularizers and parameters, DKM-based compression keeps the original loss function and model architecture fixed. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks. Our results demonstrate that DKM delivers superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression can offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB model size (29.4x model compression factor). For MobileNet-v1, which is a challenging DNN to compress, DKM delivers 63.9% top-1 ImageNet1k accuracy with 0.72 MB model size (22.4x model compression factor). This result is 6.8% higher top-1accuracy and 33% relatively smaller model size than the current state-of-the-art DNN compression algorithms. Additionally, DKM enables compression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on GLUE NLP benchmarks.


## Introduction

이 논문에서 말하는 주요 Contribution은 3가지이다.
* Deep Learning을 위한 새로운 미분 가능한 K-means 클러스터링 제안
* DKM의 다차원 확장성과 그 효율, 성능 달성
* DKM을 적용하여 DNN 모델을 압축하고 Vision, NLP 모두 sota 달성


## Algorithm

### Motivation



### DKM

### DKM in Multi-dimension